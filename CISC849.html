<!DOCTYPE HTML>
<html>

  <head>
    <title>Haibo Yang</title>
    <meta name="description" content="Homepage for Haibo Yang" />
    <meta name="keywords" content="Haibo Yang, homepage, PhD, computer science, AI, machine learning, federated learning" />
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <link rel="stylesheet" type="text/css" href="css/style.css" />
    <link rel="stylesheet" href="css/social-circles.css"> 
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <!-- modernizr enables HTML5 elements and feature detects -->
    <script type="text/javascript" src="js/modernizr-1.5.min.js"></script>
  
  </head>

<body>
  <div id="main">
    <header>
      <div id="logo"><h1>Haibo<a href="#">&nbsp;Yang</a></h1></div>
      <nav>
        <ul class="lavaLampWithImage" id="lava_menu">
          <li><a href="index.html">home</a></li>
          <li><a href="publications.html">publications</a></li>
          <li class="current"><a href="teaching.html">teaching</a></li>
          <li><a href="blogs.html">blogs</a></li>
          <!-- <li><a href="another_page.html">another page</a></li>
          <li><a href="contact.php">contact</a></li> -->
        </ul>
      </nav>
    </header>    
    <div id="site_content">
    <!-- CSCI 635 Introduction to Machine Learning-->
      <table width="100%" cellspacing="0" cellpadding="0" border="0">
        <tbody>
          <tr>
            <td width="100%" valign="bottom"><a name="overview"></a><font
                size="5"><b><font
                    face="Arial, Helvetica, sans-serif">
                    <center>CISC 849 PhD Seminar: Non-Convex Optimization for Modern Machine Learning <br />
                      <sub><sup>(Spring 2024)</sup></sub></center>
                  </font></b></font></td>
          </tr>
        </tbody>
      </table>
      <!--  -->
      <h2> Personnel</h2>
                    <p> <b>Instructor</b>: Haibo Yang, Assistant
                      Professor, Dept. of Computing and Information Sciences Ph.D.<br />
                      <b>Contact:</b> Rm 74-1073, <a href="mailto:hbycis@rit.edu">hbycis@rit.edu</a><br />
                      <b>Time &amp; Location: </b> TuTh 2:00PM -- 3:15PM, Golisano Hall (GOL)-2455  <br />
                      <b>Office Hours:</b> Th 3:15PM – 4:15PM <br />
                      <!-- <b>TA:</b> Aarti Ganesh Nayak, an5265@rit.edu <br />
                      <b>TA Hours:</b> Mon 2:00PM — 3:00PM </p> -->


      <h2>Course Description</h2>
      <p>This course will introduce algorithm design and convergence analysis in non-convex optimization, with a strong emphasis on their practical applications in addressing contemporary challenges in machine learning and data science. The goal of this course is to prepare graduate students with a solid theoretical foundation at the intersection of optimization and machine learning so that they will be able to use optimization to solve advanced machine learning problems and/or conduct advanced research in the related fields. This course will focus on topics in nonconvex optimization that are of special interest in the machine learning community. Topics covered include large-scale distributed learning (for foundation models and large language models), federated learning, multi-task learning, as well as private and robust machine learning.</p>
      <h2>Course Materials</h2>
      <p>There is no required textbook. 
        Most of the material covered in the class will be based on recently published papers, relevant monographs, or classical books. 
        A list of recently trending or historically important papers on nonconvex optimization theory for  machine learning will be provided.</p>
        <h2>Paper Reading Assignments</h2>
        <p>There will be estimated three paper reading assignments, each of which will be assigned during each topic set. Reading assignment must be typeset in <strong>NeurIPS</strong> format.</p>
        <p>In each reading assignment, each student writes a review of a set of related papers in a topic set published in recent major machine learning venues (e.g., ICML, NeurIPS, ICLR, AAAI) or on arXiv. Some papers may be from those lectured in class.</p>
        <p>The reviews may include the following: 1) a summary of the papers and their connections/relationships; 2) strengths/weaknesses of the papers from the following aspects: soundness of assumptions/theorems, empirical evaluation, novelty, and significance, etc.; 3) which parts are difficult to understand, questions about proofs/results/experiments (if there are any); and 4) how the papers can be improved and extended.
        </p>
        <h2>Final Project</h2>
        <p>Students could choose to finish a project individually or by a team of no more than two persons. Final reports will be due after project presentations in the final week. Final reports should follow the <strong>NeurIPS format</strong>. Each project is required to have a 20-minute presentation in the final week. Attendance to your fellow students' presentations is required. Potential project ideas include but are not limited to: 1) nontrivial extension of the results introduced in class; 2) novel applications in your own research area; 3) new theoretical analysis of an existing algorithm, etc.
        </p>
        <p>Each project should contain something new. It is important that you justify its novelty.</p>
        
        <!-- <h4>Recommended Textbooks</h4>
        <ur>
          <li>Hastie, T., et al. (2009) The elements of statistical learning, 2nd Edition. Springer.</li>
          <li>Charniak, E. (2019) An Introduction to Deep Learning.</li>
        </ur>
        <h4>Additional References</h4>
        <ur>
          <li>Francis Bach. (2023) Learning Theory from First Principles.</li>
          <li>Ian Goodfellow, et al. (2016) Deep Learning, MIT press, 2016.</li>
        </ur> -->
      <h2>Prerequisites</h2>
        <p>Working knowledge of probability and linear algebra. Prior exposure to convex/nonlinear optimization is a plus but not necessary.</p>
      <h2>Grading Policy</h2>
        <p>Class Participation: 10%; Paper Reading Assignments: 45%; Project: 45%.</p>
        <h2>Schedule</h2>
        </center>
        <ul>
          <li><strong>Week 1--2: Foundamentals of Optimization</strong>
            <ul>
              <li>Basic analysis and linear algebra</li>
              <li>Foundations of convex analysis</li>
              <li>Concept of convergence and its metrics</li>
            </ul>
          </li>
        
          <li><strong>Weeks 2--7: First-Order Methods</strong>
            <ul>
              <li>Gradient descent and stochastic gradient methods</li>
              <li>Variance-reduction methods</li>
              <li>Adaptive methods</li>
              <li>Parameter-free methods</li>
              <li><strong>Case study:</strong>  algorithm design in deep learning</li>
            </ul>
          </li>
        
          <li><strong>Weeks 8--12: Distributed and Federated Learning</strong>
            <ul>
              <li>Synchronous and asynchronous SGD</li>
              <li>Local update SGD and federated learning</li>
              <li>Communication efficient methods: quantization and sparsification</li>
              <li><strong>Case study:</strong> large-scale distributed learning for large language models (LLM); differentially private distributed learning; robust distributed learning</li>
            </ul>
          </li>
        
          <li><strong>Weeks 13--16: Multi-Objective Learning</strong>
            <ul>
              <li>Classical MOO: weighted sum and constraint methods</li>
              <li>Multiple (stochastic) gradient descent methods</li>
              <li><strong>Case study:</strong> multi-task learning</li>
            </ul>
          </li>
        </ul>        
        <!-- <table border="1">
                  <tr>
                      <th>Week/Start</th>
                      <th>Topics</th>
                      <th>Others</th>
                  </tr>
                  <tr>
                      <td>1 (8/28)</td>
                      <td>Machine learning, classification, regression</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>2 (9/04)</td>
                      <td>Linear models, Least Squares</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>3 (9/11)</td>
                      <td>KNN, SVM</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>4 (9/18)</td>
                      <td>Kernel</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>5 (9/25)</td>
                      <td>Generative learning, GDA</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>6 (10/02)</td>
                      <td>Navie Bayes, Perceptron Learning, Neural Networks</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>7 (10/09)</td>
                      <td>Backpropagation</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>8 (10/16)</td>
                      <td>Bias,variance, regularization</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>9 (10/23)</td>
                      <td>Decision Tree,ensembles</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>10 (10/30)</td>
                      <td>Project Proposal Presentations</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>11 (11/06)</td>
                      <td>Unsupervised learning: clustering, K-means</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>12 (11/13)</td>
                      <td>EM, PCA</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>-- (11/20)</td>
                      <td>THANKSGIVING WEEK – NO CLASSES</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>13 (11/27)</td>
                      <td>Self-supervised learning</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>14 (12/04)</td>
                      <td>Reinforcement learning</td>
                      <td></td>
                  </tr>
                  <tr>
                      <td>15 (12/11)</td>
                      <td>Finals</td>
                      <td></td>
                  </tr> 
        </table> -->
        <p></p>
        <h2>Academic Integrity</h2>
        <p>As an institution of higher learning, RIT expects students to behave honestly and ethically at all times, especially when submitting work for evaluation in conjunction with any course or degree requirement. The  Golisano College of Computing and Information Sciences encourages all students to become familiar with the RIT Honor Code and with RIT's Academic Integrity Policy. Students may discuss assignments with others including classmates, tutors and SLI instructors. After any such discussions, students must discard all written notes/pictures/etc. Submitting any work written by others or as an unsanctioned team is considered an act of academic dishonesty. Team-developed work also must be created solely by the team members and not copied from others or other sources. Work copied from Github or other similar sources will be subject to prosecution for breach of academic integrity.</p>
  </div>
  <!-- javascript at the bottom for fast page loading -->
  <script type="text/javascript" src="js/jquery.min.js"></script>
  <script type="text/javascript" src="js/jquery.easing.min.js"></script>
  <script type="text/javascript" src="js/jquery.lavalamp.min.js"></script>
  <script type="text/javascript" src="js/jquery.kwicks-1.5.1.js"></script>
  <script type="text/javascript">
    $(document).ready(function() {
      $('#images').kwicks({
        max : 600,
        spacing : 2
      });
    });
  </script>
  <script type="text/javascript">
    $(function() {
      $("#lava_menu").lavaLamp({
        fx: "backout",
        speed: 700
      });
    });
  </script>
</body>
</html>
